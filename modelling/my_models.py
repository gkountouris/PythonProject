import torch.nn as nn


class OnTopModeler(nn.Module):
    def __init__(self, input_size, hidden_nodes):
        super(OnTopModeler, self).__init__()
        self.input_size = input_size
        self.linear1 = nn.Linear(input_size, hidden_nodes, bias=True)
        self.linear2 = nn.Linear(hidden_nodes, 2, bias=True)
        self.loss = nn.BCELoss()
        self.tanh = nn.Tanh()

    def forward(self, input_xs):
        y = self.linear1(input_xs)
        y = self.tanh(y)
        y = self.linear2(y)
        return y

# class MultiHeadAttention(nn.Module):
#     """
#     Multi-Head Attention proposed in "Attention Is All You Need"
#     Instead of performing a single attention function with d_model-dimensional keys, values, and queries,
#     project the queries, keys and values h times with different, learned linear projections to d_head dimensions.
#     These are concatenated and once again projected, resulting in the final values.
#     Multi-head attention allows the model to jointly attend to information from different representation
#     subspaces at different positions.
#     MultiHead(Q, K, V) = Concat(head_1, ..., head_h) 路 W_o
#         where head_i = Attention(Q 路 W_q, K 路 W_k, V 路 W_v)
#     Args:
#         d_model (int): The dimension of keys / values / quries (default: 512)
#         num_heads (int): The number of attention heads. (default: 8)
#     Inputs: query, key, value, mask
#         - **query** (batch, q_len, d_model): In transformer, three different ways:
#             Case 1: come from previoys decoder layer
#             Case 2: come from the input embedding
#             Case 3: come from the output embedding (masked)
#         - **key** (batch, k_len, d_model): In transformer, three different ways:
#             Case 1: come from the output of the encoder
#             Case 2: come from the input embeddings
#             Case 3: come from the output embedding (masked)
#         - **value** (batch, v_len, d_model): In transformer, three different ways:
#             Case 1: come from the output of the encoder
#             Case 2: come from the input embeddings
#             Case 3: come from the output embedding (masked)
#         - **mask** (-): tensor containing indices to be masked
#     Returns: output, attn
#         - **output** (batch, output_len, dimensions): tensor containing the attended output features.
#         - **attn** (batch * num_heads, v_len): tensor containing the attention (alignment) from the encoder outputs.
#     """
#     def __init__(self, d_model: int = 512, num_heads: int = 8):
#         super(MultiHeadAttention, self).__init__()
#
#         assert d_model % num_heads == 0, "d_model % num_heads should be zero."
#
#         self.d_head = int(d_model / num_heads)
#         self.num_heads = num_heads
#         self.scaled_dot_attn = ScaledDotProductAttention(self.d_head)
#         self.query_proj = nn.Linear(d_model, self.d_head * num_heads)
#         self.key_proj = nn.Linear(d_model, self.d_head * num_heads)
#         self.value_proj = nn.Linear(d_model, self.d_head * num_heads)
#
#     def forward(
#             self,
#             query: Tensor,
#             key: Tensor,
#             value: Tensor,
#             mask: Optional[Tensor] = None
#     ) -> Tuple[Tensor, Tensor]:
#         batch_size = value.size(0)
#
#         query = self.query_proj(query).view(batch_size, -1, self.num_heads, self.d_head)  # BxQ_LENxNxD
#         key = self.key_proj(key).view(batch_size, -1, self.num_heads, self.d_head)      # BxK_LENxNxD
#         value = self.value_proj(value).view(batch_size, -1, self.num_heads, self.d_head)  # BxV_LENxNxD
#
#         query = query.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.d_head)  # BNxQ_LENxD
#         key = key.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.d_head)      # BNxK_LENxD
#         value = value.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.d_head)  # BNxV_LENxD
#
#         if mask is not None:
#             mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1)  # BxNxQ_LENxK_LEN
#
#         context, attn = self.scaled_dot_attn(query, key, value, mask)
#
#         context = context.view(self.num_heads, batch_size, -1, self.d_head)
#         context = context.permute(1, 2, 0, 3).contiguous().view(batch_size, -1, self.num_heads * self.d_head)  # BxTxND
#
#         return context, attn
